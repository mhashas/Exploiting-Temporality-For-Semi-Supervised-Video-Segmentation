import copy
from tqdm import tqdm
import torch
import torch.nn as nn
import numpy as np

from dataloader.cityscapes import CityScapes
from util.general_functions import get_model, get_optimizer, make_data_loader, plot_grad_flow, calc_width, count_parameters
from util.lr_scheduler import LR_Scheduler
from util.losses import get_loss_function, get_reconstruction_loss_function
from util.class_weighting import get_class_weights
from util.evaluator import Evaluator
from util.summary import TensorboardSummary

class Trainer(object):

    def __init__(self, args):
        self.args = args
        self.tr_global_step = 1
        self.val_global_step = 1
        self.best_mIoU = 0
        self.num_classes = CityScapes.num_classes
        self.mode = args.mode
        self.segmentation = args.segmentation
        self.reconstruct = args.reconstruct

        self.model = get_model(args)
        self.best_model = copy.deepcopy(self.model)
        self.optimizer = get_optimizer(self.model, args)
        self.summary = TensorboardSummary(args)

        if not args.trainval:
            self.train_loader, self.val_loader = make_data_loader(args, 'train'), make_data_loader(args, 'val')
        else:
            self.train_loader, self.val_loader = make_data_loader(args, 'trainval'), make_data_loader(args, 'test')

        self.class_weights = get_class_weights(self.train_loader, self.num_classes, args.weighting_mode) if args.use_class_weights else None
        self.criterion = get_loss_function(args.loss_type, self.class_weights)
        if self.reconstruct:
            self.reconstruction_criterion = get_reconstruction_loss_function(args.reconstruct_loss_type)
        self.scheduler = LR_Scheduler(args.lr_policy, args.lr, args.epochs, len(self.train_loader))
        self.evaluator = Evaluator(self.num_classes)


    def training(self, epoch):
        train_loss = 0.0
        if self.reconstruct:
            train_reconstruction_loss = 0.0
        self.model.train()
        tbar = tqdm(self.train_loader)
        num_img_tr = len(self.train_loader)

        for i, sample in enumerate(tbar):
            with torch.autograd.set_detect_anomaly(True):
                image = sample[0]
                target = sample[1]

                if self.args.cuda:
                    image, target = image.cuda(), target.cuda()

                reconstruction_target = None
                if self.reconstruct:
                    reconstruction_target = sample[2].cuda() if self.args.cuda else sample[2]

                self.scheduler(self.optimizer, i, epoch, self.best_mIoU)
                self.optimizer.zero_grad()

                segmentation_output, reconstruction_output = self.model(image)

                reconstruction_output_GT, reconstruction_target_GT, segmentation_output_GT = None, None, None
                if 'sequence' in self.mode:
                    image_GT = image[:, self.args.timesteps - 1, :, :, :]

                    if self.segmentation:
                        segmentation_output_GT = segmentation_output[:, self.args.timesteps - 1, :, :, :]

                    if self.reconstruct:
                        reconstruction_output_GT = reconstruction_output[:, self.args.timesteps - 1, :, :, :]
                        reconstruction_target_GT = reconstruction_target[:, self.args.timesteps - 1, :, :, :]
                else:
                    image_GT, segmentation_output_GT = image, segmentation_output
                    if self.mode == 'fbf-1234':
                        image_GT = image_GT[:, :3, : ,:]

                if self.segmentation:
                    segmentation_loss = self.criterion(segmentation_output_GT, target.long())
                    total_loss = segmentation_loss

                if self.reconstruct:
                    reconstruction_loss = self.reconstruction_criterion(reconstruction_output, reconstruction_target)
                    train_reconstruction_loss += reconstruction_loss.item()
                    reconstruction_loss = self.args.reconstruct_loss_coeff * reconstruction_loss
                    total_loss = reconstruction_loss

                if self.segmentation and self.reconstruct:
                    total_loss = segmentation_loss + reconstruction_loss

                total_loss.backward()
                #plot_grad_flow(self.model)

                # Show 10 * 3 inference results each epoch
                if i % (num_img_tr // 10) == 0:
                    self.summary.visualize_image(self.tr_global_step, image_GT, target, segmentation_output_GT,
                                                 reconstruction_output_GT, reconstruction_target_GT, split="train")
                    self.tr_global_step += 1

                if self.args.clip > 0:
                    if self.args.gpu_ids:
                        torch.nn.utils.clip_grad_norm_(self.model.module().parameters(), self.args.clip)
                    else:
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.clip)
                self.optimizer.step()

                train_loss += total_loss.item()
                tbar.set_description('Train loss: %.3f' % (train_loss / (i + 1)))

        self.summary.add_scalar('train/total_loss_epoch', train_loss, epoch)
        if self.reconstruct:
            self.summary.add_scalar('train/total_recon_loss_epoch', train_reconstruction_loss, epoch)
            self.summary.add_scalar('train/total_seg_loss_epoch', train_loss - train_reconstruction_loss, epoch)
        print('[Epoch: %d, numImages: %5d]' % (epoch, i * self.args.batch_size + image.data.shape[0]))

    def validation(self, epoch):
        test_loss = 0.0
        test_reconstruction_loss = 0.0
        self.model.eval()
        vbar = tqdm(self.val_loader)
        num_img_val = len(self.val_loader)

        labels = []
        outputs = []

        for i, sample in enumerate(vbar):  # inner loop within one epoch
            image = sample[0]
            target = sample[1]

            if self.args.cuda:
                image, target = image.cuda(), target.cuda()

            reconstruction_target = None
            if self.reconstruct:
                reconstruction_target = sample[2].cuda() if self.args.cuda else sample[2]

            with torch.no_grad():
                segmentation_output, reconstruction_output = self.model(image)

            reconstruction_output_GT, reconstruction_target_GT, segmentation_output_GT = None, None, None
            if 'sequence' in self.mode:
                image_GT = image[:, self.args.timesteps - 1, :, :, :]

                if self.segmentation:
                    segmentation_output_GT = segmentation_output[:, self.args.timesteps - 1, :, :, :]

                if self.reconstruct:
                    reconstruction_output_GT = reconstruction_output[:, self.args.timesteps - 1, :, :, :]
                    reconstruction_target_GT = reconstruction_target[:, self.args.timesteps - 1, :, :, :]
            else:
                image_GT, segmentation_output_GT = image, segmentation_output
                if self.mode == 'fbf-1234':
                    image_GT = image_GT[:, :3, :, :]

            if self.segmentation:
                segmentation_loss = self.criterion(segmentation_output_GT, target.long())
                total_loss = segmentation_loss

            if self.reconstruct:
                reconstruction_loss = self.reconstruction_criterion(reconstruction_output, reconstruction_target)
                test_reconstruction_loss += reconstruction_loss.item()
                reconstruction_loss = self.args.reconstruct_loss_coeff * reconstruction_loss
                total_loss = reconstruction_loss

            if self.segmentation and self.reconstruct:
                total_loss = reconstruction_loss + segmentation_loss

            # Show 10 * 3 inference results each epoch
            if i % (num_img_val // 10) == 0:
                self.summary.visualize_image(self.val_global_step, image_GT, target, segmentation_output_GT, reconstruction_output_GT, reconstruction_target_GT, split="val")
                self.val_global_step += 1

            test_loss += total_loss.item()
            vbar.set_description('Val loss: %.3f' % (test_loss / (i + 1)))

            if self.segmentation:
                outputs.append(torch.argmax(segmentation_output_GT, dim=1).cpu().numpy())
                labels.append(target.cpu().numpy())

        if self.segmentation:
            acc, acc_cls, mIoU, IoU_class, fwavacc = self.evaluator.evaluate(outputs, labels)
            self.summary.add_results(epoch, mIoU, acc, acc_cls, fwavacc, test_loss, split="val")

        if self.reconstruct:
            self.summary.add_scalar('val/total_recon_loss_epoch', test_reconstruction_loss, epoch)
            self.summary.add_scalar('val/total_seg_loss_epoch', test_loss - test_reconstruction_loss, epoch)

        if self.segmentation and mIoU > self.best_mIoU:
            self.best_mIoU = mIoU
            self.best_model = copy.deepcopy(self.model)

        if self.segmentation:
            print(20 * "-")
            for i in range(len(IoU_class)):
                print("IoU for class " + CityScapes.classes[i] + " is: " + str(IoU_class[i]))
            print(20 * "-")
            print("Accuray: ", acc)
            print("Class accuracy: ", acc_cls)
            print("FwIoU:", fwavacc)
            print("Mean IoU: ", mIoU)
            print("Best IoU: ", self.best_mIoU)

    def visualization(self, split):
        print(20 * "-")
        print("Started final Visualization")
        print(20 * "-")

        visualization_loss = 0.0
        visualization_reconstruction_loss = 0.0

        self.best_model.eval()
        if split == 'test' or split == 'demoVideo':
            vis_bar = tqdm(make_data_loader(self.args, split))
        else:
            vis_bar = tqdm(self.val_loader)

        labels = []
        outputs = []
        paths = []

        for i, sample in enumerate(vis_bar):  # inner loop within one epoch
            image = sample[0]
            target = sample[1]

            if self.args.cuda:
                image, target = image.cuda(), target.cuda()

            reconstruction_target = None
            if self.reconstruct:
                reconstruction_target = sample[2].cuda() if self.args.cuda else sample[2]

            with torch.no_grad():
                segmentation_output, reconstruction_output = self.best_model(image)

            reconstruction_output_GT, reconstruction_target_GT = None, None
            if 'sequence' in self.mode:
                image_GT = image[:, self.args.timesteps - 1, :, :, :]
                segmentation_output_GT = segmentation_output[:, self.args.timesteps - 1, :, :, :]
                if self.reconstruct:
                    reconstruction_output_GT = reconstruction_output[:, self.args.timesteps - 1, :, :, :]
                    reconstruction_target_GT = reconstruction_target[:, self.args.timesteps - 1, :, :, :]
            else:
                image_GT, segmentation_output_GT = image, segmentation_output
                if self.mode == 'fbf-1234':
                    image_GT = image_GT[:, :3, :, :]

            segmentation_loss = self.criterion(segmentation_output_GT, target.long())

            if self.reconstruct:
                reconstruction_loss = self.reconstruction_criterion(reconstruction_output, reconstruction_target)
                visualization_reconstruction_loss += reconstruction_loss.item()
                reconstruction_loss = self.args.reconstruct_loss_coeff * reconstruction_loss
                total_loss = reconstruction_loss + segmentation_loss
            else:
                total_loss = segmentation_loss

            visualization_loss += total_loss.item()
            vis_bar.set_description('Visualization loss: %.3f' % (visualization_loss / (i + 1)))

            outputs.append(torch.argmax(segmentation_output_GT, dim=1).cpu())
            labels.append(target.cpu())

            if split == 'test' or split == 'demoVideo':
                paths.append(sample[2][0])

        if split not in ['test', 'demoVideo']:
            acc, acc_cls, mIoU, IoU_class, fwavacc = self.evaluator.evaluate([output.numpy() for output in outputs], [label.numpy() for label in labels])

        outputs = torch.squeeze(torch.stack(outputs))
        labels = torch.squeeze(torch.stack(labels))
        self.summary.save_visualization_images(outputs, labels, paths)

        if split not in ['test', 'demoVideo']:
            print(20 * "-")
            for i in range(len(IoU_class)):
                print("IoU for class " + CityScapes.classes[i] + " is: " + str(IoU_class[i]))
            print(20 * "-")
            print("Accuray: ", acc)
            print("Class accuracy: ", acc_cls)
            print("FwIoU:", fwavacc)
            print("Mean IoU: ", mIoU)
            print("Best IoU: ", self.best_mIoU)

    def save_network(self):
        self.summary.save_network(self.best_model)

    def load_network(self):
        self.best_model = get_model(self.args)
        self.best_model.load_state_dict(torch.load(''))
